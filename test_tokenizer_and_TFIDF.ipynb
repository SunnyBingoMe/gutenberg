{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is running in jupyter with spark context, which can also be running directly in `pyspark`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Ref https://www.linkedin.com/learning/spark-for-machine-learning-ai/tokenize-text-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = spark.createDataFrame(\n",
    "    [\n",
    "    (1, \"One thesis thesis.\"),\n",
    "    (2, \"The thesis My thesis includes methods to predict traffic\"),\n",
    "    (3, \"It also contains supporting tools for future pipelines\"),\n",
    "    (4, \"Short dot.\")\n",
    "    ],\n",
    "    [\"id\", \"sentence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|  One thesis thesis.|\n",
      "|  2|The thesis My the...|\n",
      "|  3|It also contains ...|\n",
      "|  4|          Short dot.|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer \n",
    "(split sentences into lower-case words)\n",
    "\n",
    "**Note** that the period (.) is connected to the last word, which is NOT wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|  One thesis thesis.|[one, thesis, the...|\n",
      "|  2|The thesis My the...|[the, thesis, my,...|\n",
      "|  3|It also contains ...|[it, also, contai...|\n",
      "|  4|          Short dot.|       [short, dot.]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "sent_token = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)\n",
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *tf*:\n",
    "\n",
    "TF maps words to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='One thesis thesis.')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='One thesis thesis.', words=['one', 'thesis', 'thesis.'])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=10, inputCol='words', outputCol='rawFeatures')\n",
    "sent_hashTF_df = hashingTF.transform(sent_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='One thesis thesis.', words=['one', 'thesis', 'thesis.'], rawFeatures=SparseVector(10, {0: 1.0, 1: 1.0, 4: 1.0})),\n",
       " Row(id=2, sentence='The thesis My thesis includes methods to predict traffic', words=['the', 'thesis', 'my', 'thesis', 'includes', 'methods', 'to', 'predict', 'traffic'], rawFeatures=SparseVector(10, {0: 3.0, 2: 1.0, 3: 1.0, 4: 1.0, 6: 1.0, 8: 1.0, 9: 1.0}))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hashTF_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation** In row 2, two words 'thesis's are hashed to index '0', but another word is also hashed to '0', so index '0' got hit 3 times.\n",
    "\n",
    "**Note** In rwo 1, the two terms 'thesis' and 'thesis.' are treated differently due to the '.' char, so be careful whether this is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *idf:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = IDF(inputCol='rawFeatures').fit(sent_hashTF_df)\n",
    "tfidf_df = idfModel.transform(sent_hashTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='One thesis thesis.', words=['one', 'thesis', 'thesis.'], rawFeatures=SparseVector(10, {0: 1.0, 1: 1.0, 4: 1.0}), IDF_493fb1d2efd8f3d32d6b__output=SparseVector(10, {0: 0.2231, 1: 0.9163, 4: 0.2231})),\n",
       " Row(id=2, sentence='The thesis My thesis includes methods to predict traffic', words=['the', 'thesis', 'my', 'thesis', 'includes', 'methods', 'to', 'predict', 'traffic'], rawFeatures=SparseVector(10, {0: 3.0, 2: 1.0, 3: 1.0, 4: 1.0, 6: 1.0, 8: 1.0, 9: 1.0}), IDF_493fb1d2efd8f3d32d6b__output=SparseVector(10, {0: 0.6694, 2: 0.5108, 3: 0.5108, 4: 0.2231, 6: 0.5108, 8: 0.5108, 9: 0.9163}))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** If the IDF()'s param 'outputCol' is not specified, a random named column is added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try The Real-World Documents\n",
    "\n",
    "See file `TFIDF_Kmeans.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
